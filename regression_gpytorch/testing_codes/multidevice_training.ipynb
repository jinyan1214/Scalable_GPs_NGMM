{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import shutil\n",
    "import torch\n",
    "sys.path.append(\"/home/jyzhao/python_env/gpytorch_local/gpytorch\")\n",
    "sys.path.append(\"/home/jyzhao/python_env/gpytorch_local/linear_operator\")\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import sys\n",
    "sys.path.append(\"/resnick/groups/enceladus/jyzhao/Scalable_GPs_jz/regression_gpytorch/src\")\n",
    "from training_utils.GPModel import MultiDeviceExactGPModel\n",
    "from training_utils.utils import set_seed, read_hdf5, predict_dataset\n",
    "from training_utils.LBFGS import FullBatchLBFGS\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec96c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(training_loss, model, save_dir):\n",
    "    fig, axes = plt.subplots(1,3,figsize=(16, 4))\n",
    "    for label, color in zip(['source', 'site', 'path'], ['blue', 'red', 'green']):\n",
    "        if label in model.kernels:\n",
    "            axes[0].plot(getattr(model, f\"{label}_len_training\"), label=f\"{label} length scale\", c = color)\n",
    "    for label, color in zip(['source', 'site', 'path'], ['blue', 'red', 'green']):\n",
    "        if label in model.kernels:\n",
    "            axes[1].plot(getattr(model, f\"{label}_var_training\"), '--', label=f\"{label} variance\", c = color)\n",
    "    if 'eta' in model.kernels:\n",
    "        axes[1].plot(model.eta_training, ':', label='Between event variance')\n",
    "\n",
    "    axes[0].set_xlabel('Iteration')\n",
    "    axes[0].set_ylabel('Hyperparameter Value')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(model.likelihood_noise, ':', label='likelihood noise')\n",
    "    axes[1].set_xlabel('Iteration')\n",
    "    axes[1].set_ylabel('Hyperparameter Value')\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].set_ylim([0,10])\n",
    "    axes[2].plot(training_loss, label = \"ELBO loss\")\n",
    "    axes[2].set_xlabel('Iteration')\n",
    "    axes[2].set_ylabel('Training loss');\n",
    "    axes[2].legend()\n",
    "    \n",
    "    fig.savefig(os.path.join(save_dir, 'training_loss_parameters.png'), dpi=300)\n",
    "    plt.close(fig)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b9f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 62\n",
    "num_training_eqs = 100\n",
    "num_testing_eqs = 4179\n",
    "num_training_iter = 2000\n",
    "set_seed(random_seed)\n",
    "output_dir = \"/resnick/groups/enceladus/jyzhao/Scalable_GPs_jz/regression_gpytorch/output\"\n",
    "trained_model_dir = os.path.join(output_dir, 'trained_exact_models', f'{num_training_eqs}_training_eqs_{num_training_iter}_iters')\n",
    "if os.path.exists(trained_model_dir):\n",
    "    shutil.rmtree(trained_model_dir)\n",
    "os.makedirs(trained_model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data_file = os.path.join(output_dir,'formatted_data', \n",
    "                               f'training_{num_training_eqs}_eqs', \n",
    "                               f'training_sites_training_eqs_2.00_ASK14_{num_training_eqs}_eqs_1_var_per_eq.h5')\n",
    "train_x, train_y = read_hdf5(train_data_file)\n",
    "print(f\"Number of training samples: {train_x.shape[0]}\")\n",
    "print(f\"Number of training features: {train_x.shape[1]}\")\n",
    "# Centeralize y:\n",
    "train_y_mean = np.mean(train_y)\n",
    "train_y = train_y - train_y_mean\n",
    "train_x = torch.from_numpy(train_x)\n",
    "train_y = torch.from_numpy(train_y)\n",
    "train_x, train_y = train_x.contiguous(), train_y.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba493e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    n_devices = torch.cuda.device_count()\n",
    "    print('Planning to run on {} GPUs.'.format(n_devices))\n",
    "    output_device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    output_device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU for training\")\n",
    "train_x = train_x.to(output_device)\n",
    "train_y = train_y.to(output_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ceefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are kernel normalizers that are estimated by calculating the average\n",
    "# kernel distance from 100 training earthquakes\n",
    "source_normalizer = 92.8112\n",
    "site_normalizer = 40.3403\n",
    "path_normalizer = 76.3133\n",
    "print(f\"Kernel lengthscale normalizing constants: source {source_normalizer}, site {site_normalizer}, path {path_normalizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d716cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = MultiDeviceExactGPModel(train_x, train_y, likelihood, n_devices, \n",
    "                    output_device,\n",
    "                    source_normalizer=source_normalizer, \n",
    "                    site_normalizer=site_normalizer, \n",
    "                    path_normalizer=path_normalizer)\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = FullBatchLBFGS(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial scale for the effects based on observations in preliminary runs\n",
    "if 'path' in model.kernels:\n",
    "    var_init = 0.1\n",
    "    lengthscale = 1.2\n",
    "    kernel_ind = model.kernels.index('path')\n",
    "    model.covar_module.base_kernel.kernels[kernel_ind].outputscale = var_init\n",
    "    model.covar_module.base_kernel.kernels[kernel_ind].base_kernel.lengthscale = lengthscale\n",
    "    print(f\"Path kernel initial variance is set to {var_init}, lengthscale is set to {lengthscale}\")\n",
    "if 'source' in model.kernels:\n",
    "    var_init = 0.1\n",
    "    lengthscale = 0.15\n",
    "    kernel_ind = model.kernels.index('source')\n",
    "    model.covar_module.base_kernel.kernels[kernel_ind].outputscale = var_init\n",
    "    model.covar_module.base_kernel.kernels[kernel_ind].base_kernel.lengthscale = lengthscale\n",
    "    print(f\"Source kernel initial variance is set to {var_init}, lengthscale is set to {lengthscale}\")\n",
    "if 'site' in model.kernels:\n",
    "    var_init = 0.1\n",
    "    lengthscale = 0.3\n",
    "    kernel_ind = model.kernels.index('site')\n",
    "    model.covar_module.base_kernel.kernels[kernel_ind].outputscale = var_init\n",
    "    model.covar_module.base_kernel.kernels[kernel_ind].base_kernel.lengthscale = lengthscale\n",
    "    print(f\"Site kernel initial variance is set to {var_init}, lengthscale is set to {lengthscale}\")\n",
    "kernel_ind = model.kernels.index('eta')\n",
    "model.covar_module.base_kernel.kernels[kernel_ind].outputscale = 0.1\n",
    "likelihood.noise = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = []\n",
    "# Initialize model saving directory and save cpu state\n",
    "training_loss_file = os.path.join(trained_model_dir, 'training_hyperparameter_loss.npy')\n",
    "model_save_path_pre = os.path.join(trained_model_dir, 'cpu_model_initial.pth')\n",
    "model_state = model.state_dict()\n",
    "torch.save(model_state, model_save_path_pre)\n",
    "likelihood_save_path_pre = os.path.join(trained_model_dir, 'cpu_likelihood_initial.pth')\n",
    "likelihood_state = likelihood.state_dict()\n",
    "torch.save(likelihood_state, likelihood_save_path_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad18ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to device\n",
    "model = model.to(output_device)\n",
    "likelihood = likelihood.to(output_device)\n",
    "# Save the gpu model\n",
    "if device == torch.device(\"cuda\"):\n",
    "    model_save_path_pre = os.path.join(trained_model_dir, 'gpu_model_initial.pth')\n",
    "    model_state = model.state_dict()\n",
    "    torch.save(model_state, model_save_path_pre)\n",
    "    likelihood_save_path_pre = os.path.join(trained_model_dir, 'gpu_likelihood_initial.pth')\n",
    "    likelihood_state = likelihood.state_dict()\n",
    "    torch.save(likelihood_state, likelihood_save_path_pre)\n",
    "    model_save_path_post_gpu = os.path.join(trained_model_dir, 'gpu_model_trained.pth')\n",
    "    likelihood_save_path_post_gpu = os.path.join(trained_model_dir, 'gpu_likelihood_trained.pth')\n",
    "model_save_path_post_cpu = os.path.join(trained_model_dir, 'cpu_model_trained.pth')\n",
    "likelihood_save_path_post_cpu = os.path.join(trained_model_dir, 'cpu_likelihood_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a004eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preconditioner_size=15\n",
    "model.train()\n",
    "likelihood.train()\n",
    "smallest_loss = np.inf\n",
    "with gpytorch.settings.max_preconditioner_size(preconditioner_size):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        return loss\n",
    "    loss = closure()\n",
    "    loss.backward(retain_graph=True)\n",
    "    training_tqdm = tqdm(range(num_training_iter), leave = True)\n",
    "    for i in training_tqdm:\n",
    "        options = {'closure': closure, 'current_loss': loss, 'max_ls': 10,\n",
    "                   'retain_graph': True}\n",
    "        loss, _, _, _, _, _, _, fail = optimizer.step(options)\n",
    "        training_tqdm.set_postfix(loss=f\"{loss.item():.6f}\",\n",
    "                                  noise=f\"{likelihood.noise.item():.6f}\")\n",
    "        # source event\n",
    "        if 'source' in model.kernels:\n",
    "            kernel_idx = model.kernels.index('source')\n",
    "            model.source_len_training.append(model.covar_module.kernels[kernel_idx].base_kernel.lengthscale.item())\n",
    "            model.source_var_training.append(model.covar_module.kernels[kernel_idx].outputscale.item())\n",
    "        # site event\n",
    "        if 'site' in model.kernels:\n",
    "            kernel_idx = model.kernels.index('site')\n",
    "            model.site_len_training.append(model.covar_module.kernels[kernel_idx].base_kernel.lengthscale.item())\n",
    "            model.site_var_training.append(model.covar_module.kernels[kernel_idx].outputscale.item())\n",
    "        # path event\n",
    "        if 'path' in model.kernels:\n",
    "            kernel_idx = model.kernels.index('path')\n",
    "            model.path_len_training.append(model.covar_module.kernels[kernel_idx].base_kernel.lengthscale.item())\n",
    "            model.path_var_training.append(model.covar_module.kernels[kernel_idx].outputscale.item())\n",
    "        # within event\n",
    "        if 'eta' in model.kernels:\n",
    "            kernel_idx = model.kernels.index('eta')\n",
    "            model.eta_training.append(model.covar_module.kernels[kernel_idx].outputscale.item())\n",
    "            model.likelihood_noise.append(likelihood.noise.item())\n",
    "\n",
    "        training_loss.append(loss.item())\n",
    "\n",
    "        if loss.item() <= smallest_loss:\n",
    "            smallest_loss = loss.item()\n",
    "            if output_device == torch.device(\"cuda\"):\n",
    "                model_save_path_post = model_save_path_post_gpu\n",
    "                likelihood_save_path_post = likelihood_save_path_post_gpu\n",
    "            else:\n",
    "                model_save_path_post = model_save_path_post_cpu\n",
    "                likelihood_save_path_post = likelihood_save_path_post_cpu\n",
    "            best_epoch = i\n",
    "\n",
    "        if fail:\n",
    "            print('Convergence reached')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2334c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == torch.device(\"cuda\"):\n",
    "    model_save_path_converged = os.path.join(trained_model_dir, 'gpu_model_converged.pth')\n",
    "    model_state = model.state_dict()\n",
    "    torch.save(model_state, model_save_path_converged)\n",
    "    likelihood_save_path_converged = os.path.join(trained_model_dir, 'gpu_likelihood_converged.pth')\n",
    "    likelihood_state = likelihood.state_dict()\n",
    "    torch.save(likelihood_state, likelihood_save_path_converged)\n",
    "else:\n",
    "    model_save_path_converged = os.path.join(trained_model_dir, 'cpu_model_converged.pth')\n",
    "    model_state = model.state_dict()\n",
    "    torch.save(model_state, model_save_path_converged)\n",
    "    likelihood_save_path_converged = os.path.join(trained_model_dir, 'cpu_likelihood_converged.pth')\n",
    "    likelihood_state = likelihood.state_dict()\n",
    "    torch.save(likelihood_state, likelihood_save_path_converged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46feaaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lowest loss achieved at epoch:\", best_epoch)\n",
    "loss_array = np.array([\n",
    "    model.source_var_training,\n",
    "    model.source_len_training,\n",
    "    model.site_var_training,\n",
    "    model.site_len_training,\n",
    "    model.path_var_training,\n",
    "    model.path_len_training,\n",
    "    model.eta_training,\n",
    "    model.likelihood_noise,\n",
    "    training_loss]).T\n",
    "np.save(training_loss_file, loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evolution of the training loss and hyperparameters\n",
    "plot_training_loss(training_loss, model, trained_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's performance\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "# Predict on the training set\n",
    "start = time.time()\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False)\n",
    "train_mean, train_lower, train_upper = predict_dataset(train_loader, model, \n",
    "                                                       likelihood)\n",
    "print(f\"Time used for train set prediction: {time.time() - start :.1f} s\")\n",
    "train_x,train_y = train_x.cpu(), train_y.cpu()\n",
    "train_mean, train_lower, test_upper = train_upper.cpu(), train_lower.cpu(), train_upper.cpu()\n",
    "RMSE_train = np.sqrt(np.mean((train_y.numpy() - train_mean.numpy())**2))\n",
    "print(f\"Training sites Training earthquakes RMSE error {RMSE_train:.3f}; Data mean: {train_y.numpy().mean():.3f}; Data Std: {np.std(train_y.numpy()):.3f}\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "        torch.cuda.empty_cache()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb18581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test data\n",
    "test_data_file = os.path.join(output_dir,'formatted_data', \n",
    "                               f'testing_{num_testing_eqs}_eqs', \n",
    "                               f'training_sites_testing_eqs_2.00_ASK14_{num_testing_eqs}_eqs_1_var_per_eq.h5')\n",
    "test_x, test_y = read_hdf5(test_data_file)\n",
    "print(f\"Number of testing samples: {test_x.shape[0]}\")\n",
    "print(f\"Number of testing features: {test_x.shape[1]}\")\n",
    "# Centeralize y:\n",
    "test_y = test_y - train_y_mean\n",
    "test_x = torch.from_numpy(test_x).to(device)\n",
    "test_y = torch.from_numpy(test_y).to(device)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
    "# Predict on the test set\n",
    "start = time.time()\n",
    "test_mean, test_lower, test_upper = predict_dataset(test_loader, model, \n",
    "                                                    likelihood, contiguous=True)\n",
    "print(f\"Time used for test set prediction: {time.time() - start :.1f} s\")\n",
    "test_x,test_y = test_x.cpu(), test_y.cpu()\n",
    "test_mean, test_lower, test_upper = test_mean.cpu(), test_lower.cpu(), test_upper.cpu()\n",
    "RMSE_test = np.sqrt(np.mean((test_y.numpy() - test_mean.numpy())**2))\n",
    "print(f\"Training sites Testing earthquakes RMSE error {RMSE_test:.3f}; Data mean: {test_y.numpy().mean():.3f}; Data Std: {np.std(test_y.numpy()):.3f}\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d26a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training earthquake test sites data\n",
    "test_sites_train_eqs_file = os.path.join(output_dir, 'formatted_data',\n",
    "                                         f'training_{num_training_eqs}_eqs',\n",
    "                                         f'testing_sites_training_eqs_2.00_ASK14_{num_training_eqs}_eqs_1_var_per_eq.h5')\n",
    "test_sites_train_x, test_sites_train_y = read_hdf5(test_sites_train_eqs_file)\n",
    "print(f\"Number of training earthquake test sites: {test_sites_train_x.shape[0]}\")\n",
    "print(f\"Number of training earthquake test features: {test_sites_train_x.shape[1]}\")\n",
    "# Centeralize y:\n",
    "test_sites_train_y = test_sites_train_y - train_y_mean\n",
    "test_sites_train_x = torch.from_numpy(test_sites_train_x).to(device)\n",
    "test_sites_train_y = torch.from_numpy(test_sites_train_y).to(device)\n",
    "test_sites_train_dataset = TensorDataset(test_sites_train_x, test_sites_train_y)\n",
    "test_sites_train_loader = DataLoader(test_sites_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Predict on the training earthquake test sites\n",
    "start = time.time()\n",
    "test_sites_train_mean, test_sites_train_lower, test_sites_train_upper = \\\n",
    "    predict_dataset(test_sites_train_loader, model, likelihood)\n",
    "print(f\"Time used for test sites training set prediction: {time.time() - start :.1f} s\")\n",
    "test_sites_train_x,test_sites_train_y = test_sites_train_x.cpu(), test_sites_train_y.cpu()\n",
    "test_sites_train_mean, test_sites_train_lower, test_sites_train_upper = \\\n",
    "    test_mean.cpu(), test_lower.cpu(), test_upper.cpu()\n",
    "RMSE_new_sites_train = np.sqrt(np.mean((test_sites_train_y.numpy() - \n",
    "                                            test_sites_train_mean.numpy())**2))\n",
    "print(f\"Testing Sites Training earthquakes RMSE error {RMSE_new_sites_train:.3f}; Data mean: {test_sites_train_y.numpy().mean():.3f}; Data Std: {np.std(test_sites_train_y.numpy()):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpytorch_local (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
